{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Logistic Regression\n",
    "\n",
    "We will explore diﬀerent methods for improving classiﬁcation performance in the presence of class imbalance. We focus on the ‘vowel’ dataset where the proportion of the positive class is approx 10%. All models should be trained using logistic regression and the metric for comparison will be the f1 score. Train the following policies on each fold and report the mean(std-dev) f1 score for all policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import metrics\n",
    "import numpy.random as npr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"vowel_train1.csv\", header = None)\n",
    "train2 = pd.read_csv(\"vowel_train2.csv\", header = None)\n",
    "train3 = pd.read_csv(\"vowel_train3.csv\", header = None)\n",
    "train4 = pd.read_csv(\"vowel_train4.csv\", header = None)\n",
    "train5 = pd.read_csv(\"vowel_train5.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_lb1 = pd.read_csv(\"vowel_tr_label1.csv\", header = None)\n",
    "tr_lb2 = pd.read_csv(\"vowel_tr_label2.csv\", header = None)\n",
    "tr_lb3 = pd.read_csv(\"vowel_tr_label3.csv\", header = None)\n",
    "tr_lb4 = pd.read_csv(\"vowel_tr_label4.csv\", header = None)\n",
    "tr_lb5 = pd.read_csv(\"vowel_tr_label5.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = pd.read_csv(\"vowel_test1.csv\", header = None)\n",
    "test2 = pd.read_csv(\"vowel_test2.csv\", header = None)\n",
    "test3 = pd.read_csv(\"vowel_test3.csv\", header = None)\n",
    "test4 = pd.read_csv(\"vowel_test4.csv\", header = None)\n",
    "test5 = pd.read_csv(\"vowel_test5.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_lb1 = pd.read_csv(\"vowel_tst_label1.csv\", header = None)\n",
    "ts_lb2 = pd.read_csv(\"vowel_tst_label2.csv\", header = None)\n",
    "ts_lb3 = pd.read_csv(\"vowel_tst_label3.csv\", header = None)\n",
    "ts_lb4 = pd.read_csv(\"vowel_tst_label4.csv\", header = None)\n",
    "ts_lb5 = pd.read_csv(\"vowel_tst_label5.csv\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Downsampling Policy\n",
    "\n",
    "Write a function that will, given a training set, downsample the negative class samples so that the proportion of positive class samples is greater than 10%. Make this proportion a tuneable parameter of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [train1, train2, train3, train4, train5]\n",
    "tr_lb = [tr_lb1, tr_lb2, tr_lb3, tr_lb4, tr_lb5]\n",
    "test = [test1, test2, test3, test4, test5]\n",
    "ts_lb = [ts_lb1, ts_lb2, ts_lb3, ts_lb4, ts_lb5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downsample(xtrain,ytrain, p):\n",
    "    pos_nm = ytrain[0].value_counts()[1]\n",
    "    neg_nm = ytrain[0].value_counts()[0]\n",
    "    all_nm = len(ytrain)\n",
    "    posratio = float(pos_nm)/all_nm\n",
    "    new_all_nm = int(float(pos_nm)/p + 1)\n",
    "    new_neg_nm = new_all_nm - pos_nm\n",
    "    mask = ytrain[0] == 1 \n",
    "    postrain_y = ytrain[mask]\n",
    "    negtrain_y = ytrain[~mask].sample(n = new_neg_nm, random_state = 77)\n",
    "    frames_y = [postrain_y, negtrain_y]\n",
    "    new_ytrain = pd.concat(frames_y)\n",
    "    postrain_x = xtrain[mask]\n",
    "    negtrain_x = xtrain[~mask].sample(n = new_neg_nm, random_state = 77)\n",
    "    frames_x = [postrain_x, negtrain_x]\n",
    "    new_xtrain = pd.concat(frames_x)\n",
    "    return new_xtrain, new_ytrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_list = [0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3]\n",
    "f1_score_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(p_list)):\n",
    "    f1_list = []\n",
    "    for j in range(len(train)):\n",
    "        new_x, new_y = downsample(train[j], tr_lb[j], p_list[i])\n",
    "        pred = LogisticRegression().fit(new_x,new_y).predict(test[j])\n",
    "        f1 = metrics.f1_score(ts_lb[j], pred)\n",
    "        f1_list.append(f1)\n",
    "    f1_score_dict[p_list[i]] = np.mean(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: 0.76594086021505381,\n",
       " 0.11: 0.76814674256799498,\n",
       " 0.12: 0.78854978354978356,\n",
       " 0.13: 0.79391961085509466,\n",
       " 0.14: 0.80279894473442859,\n",
       " 0.15: 0.80698598892147277,\n",
       " 0.16: 0.80698598892147277,\n",
       " 0.17: 0.80163839533858494,\n",
       " 0.18: 0.81214752567693738,\n",
       " 0.19: 0.78191958191958189,\n",
       " 0.2: 0.80252913652366453,\n",
       " 0.21: 0.79317241137746575,\n",
       " 0.22: 0.79123487522940328,\n",
       " 0.23: 0.79694312047253235,\n",
       " 0.24: 0.79813739248912552,\n",
       " 0.25: 0.78091855004898492,\n",
       " 0.26: 0.76668220668220666,\n",
       " 0.27: 0.76232323232323229,\n",
       " 0.28: 0.76516011175585641,\n",
       " 0.29: 0.76808408836404352,\n",
       " 0.3: 0.77316187584480267}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Upsampling\n",
    "\n",
    "Implement a function that will upsample (via replication) the minority class so that the new class ratio is p : (1−p) where p is a tuneable parameter as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsample(xtrain, ytrain, p):\n",
    "    pos_nm = ytrain[0].value_counts()[1]\n",
    "    neg_nm = ytrain[0].value_counts()[0]\n",
    "    all_nm = len(ytrain)\n",
    "    new_pos_nm = int(float(neg_nm)*p/float(1-p))\n",
    "    mask = ytrain[0] == 0 \n",
    "    negtrain_y = ytrain[mask]\n",
    "    postrain_y = ytrain[~mask]\n",
    "    ind_pos = np.array(postrain_y.index)\n",
    "    masknew = npr.choice(ind_pos, size = new_pos_nm, replace = True)\n",
    "    postrain_y = ytrain.ix[masknew]\n",
    "    frames_y = [postrain_y, negtrain_y]\n",
    "    new_ytrain = pd.concat(frames_y)\n",
    "    postrain_x = xtrain.ix[masknew]\n",
    "    negtrain_x = xtrain[mask]\n",
    "    frames_x = [postrain_x, negtrain_x]\n",
    "    new_xtrain = pd.concat(frames_x)\n",
    "    return new_xtrain, new_ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_score_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(p_list)):\n",
    "    f1_list = []\n",
    "    for j in range(len(train)):\n",
    "        new_x, new_y = upsample(train[j], tr_lb[j], p_list[i])\n",
    "        pred = LogisticRegression().fit(new_x,new_y).predict(test[j])\n",
    "        f1 = metrics.f1_score(ts_lb[j], pred)\n",
    "        f1_list.append(f1)\n",
    "    f1_score_dict[p_list[i]] = np.mean(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: 0.75352918586789563,\n",
       " 0.11: 0.76371661168821414,\n",
       " 0.12: 0.77610743963881446,\n",
       " 0.13: 0.74255411255411263,\n",
       " 0.14: 0.80655478613732901,\n",
       " 0.15: 0.73073207443897104,\n",
       " 0.16: 0.72339080459770122,\n",
       " 0.17: 0.76257575757575757,\n",
       " 0.18: 0.74603641456582637,\n",
       " 0.19: 0.75567844342037893,\n",
       " 0.2: 0.74945115289517561,\n",
       " 0.21: 0.7759848484848485,\n",
       " 0.22: 0.78353495206436374,\n",
       " 0.23: 0.79192810457516338,\n",
       " 0.24: 0.80326270221007046,\n",
       " 0.25: 0.78239002932551327,\n",
       " 0.26: 0.80168746286393355,\n",
       " 0.27: 0.81658219495031636,\n",
       " 0.28: 0.78526544484798744,\n",
       " 0.29: 0.81831102167914305,\n",
       " 0.3: 0.79577457518633987}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Class Weighting\n",
    "\n",
    "Impose twice the weight of the majority class to the minority class. Repeat the training process with the newly designed logistic regression and report the f1 scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(train)):\n",
    "    pred = LogisticRegression(class_weight = {0:1, 1:2}).fit(train[j],tr_lb[j]).predict(test[j])\n",
    "    f1 = metrics.f1_score(ts_lb[j], pred)\n",
    "    f1_list.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78494397759103651"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Vanilla Logistic Regression\n",
    "\n",
    "Report the baseline performance of a simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(train)):\n",
    "    pred = LogisticRegression().fit(train[j],tr_lb[j]).predict(test[j])\n",
    "    f1 = metrics.f1_score(ts_lb[j], pred)\n",
    "    f1_list.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76239618094178729"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
